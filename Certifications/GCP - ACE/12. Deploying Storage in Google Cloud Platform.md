# Deploying and Managing Cloud SQL

## Cloud SQL Overview
- **Managed Relational Database Service**
- **Supported DBs:** MySQL, PostgreSQL

## Creating & Connecting to MySQL
- **Create Instance:**
  - Navigate to SQL > Create Instance > MySQL > Second Generation Instance
- **Connect:**
  - Command: `gcloud sql connect [INSTANCE_NAME] --user=root`
  - Prompt for password

## Database Operations
- **Create Database:**
  - `CREATE DATABASE [DB_NAME];`
  - `USE [DB_NAME];`
- **Create Table & Insert Data:**
  - `CREATE TABLE [TABLE_NAME] (...);`
  - `INSERT INTO [TABLE_NAME] (columns) VALUES (...);`
- **Query Data:**
  - `SELECT * FROM [TABLE_NAME];`

## Backup
- **On-Demand Backup:**
  - Console: Go to Instance > Backups > Create Backup
  - Command: `gcloud sql backups create --async --instance [INSTANCE_NAME]`
- **Automatic Backup:**
  - Console: Instance > Edit Instance > Enabled Auto Backups
  - Command: `gcloud sql instances patch [INSTANCE_NAME] --backup-start-time [HH:MM]`

# Deploying and Managing Datastore

## Adding Data
- **Create Entity:**
  - Console: Entities > Create Entity > Fill in Kind & Properties
- **Query Data:**
  - GQL (similar to SQL)

## Backup & Restore
- **Create Backup:**
  - Create Cloud Storage bucket: `gsutil mb gs://[BUCKET_NAME]/`
  - Backup Command: `gcloud datastore export --namespaces='[NAMESPACE]' gs://[BUCKET_NAME]`
- **Restore Backup:**
  - Import Command: `gcloud datastore import gs://[BUCKET]/[PATH]/[FILE].overall_export_metadata`

# Deploying and Managing BigQuery

## Estimating Cost of Queries
- **Console:** Query Editor > Check data scanned
- **Command Line:**
  - `bq --location=[LOCATION] query --use_legacy_sql=false --dry_run [SQL_QUERY]`
- **Pricing Calculator:** Estimate cost based on data size and query

## Viewing Jobs
- **Console:** BigQuery > Job History
- **Command Line:**
  - `bq --location=[LOCATION] show -j [JOB_ID]`

# Deploying and Managing Cloud Spanner

- **Create Instance:**
  1. Navigate to Cloud Spanner form
  2. Select Create Instance

- **Create Database:**
  1. Instance Details page -> Create Database
  2. Define structure using SQL DDL (CREATE TABLE, CREATE INDEX, ALTER TABLE, DROP TABLE, DROP INDEX)

- **Insert Data:**
  1. Table Details page -> Data tab
  2. Add row with columns

- **Execute Query:**
  1. Table Details page -> Query

# Deploying and Managing Cloud Pub/Sub

- **Create Topic:**
  1. Pub/Sub page -> Create Topic
  2. Name the topic

- **Create Subscription:**
  1. Topics page -> three-dot icon -> Create Subscription
  2. Specify subscription name and delivery type (pull/push)

- **Commands:**
  - `gcloud pubsub topics create [TOPIC-NAME]`
  - `gcloud pubsub subscriptions create [SUBSCRIPTION-NAME] --topic [TOPIC-NAME]`

# Deploying and Managing Cloud Bigtable

- **Create Instance:**
  1. Bigtable console -> Create Instance

- **Commands:**
  - `gcloud components update`
  - `gcloud components install cbt`
  - `echo instance = ace-exam-bigtable >> ~/.cbtrc`
  - `cbt createtable ace-exam-bt-table`
  - `cbt ls`
  - `cbt createfamily ace-exam-bt-table colfam1`
  - `cbt set ace-exam-bt-table row1 colfam1:col1=ace-exam-value`
  - `cbt read ace-exam-bt-table`

# Deploying and Managing Cloud Dataproc

- **Create Cluster:**
  1. Dataproc console -> Create Cluster
  2. Specify name, region, zone, cluster mode (single node/standard/high availability), machine configuration

- **Submit Jobs:**
  1. Cluster Details page -> Jobs form
  2. Specify cluster, job type (Spark, PySpark, SparkR, Hive, Spark SQL, Pig, Hadoop), JAR files, Main Class or JAR

- **Commands:**
  - `gcloud dataproc clusters create cluster-bc3d --zone us-west2-a`
  - `gcloud dataproc jobs submit spark --cluster cluster-bc3d --jar ace_exam_jar.jar`

- **Spark for ML:**
  - Use MLlib for machine learning on large data volumes (clustering, collaborative filtering)

# Managing Cloud Storage

- **Change Storage Class:**
  - Command: `gsutil rewrite -s [STORAGE_CLASS] gs://[PATH_TO_OBJECT]`
  - Storage Classes: multi_regional, regional, nearline, coldline

- **Move Objects:**
  - Command: `gsutil mv gs://[SOURCE_BUCKET_NAME]/[SOURCE_OBJECT_NAME] gs://[DESTINATION_BUCKET_NAME]/[DESTINATION_OBJECT_NAME]`
  - Rename Object: `gsutil mv gs://[BUCKET_NAME]/[OLD_OBJECT_NAME] gs://[BUCKET_NAME]/[NEW_OBJECT_NAME]`

- **Console Operations:**
  - Move and rename objects via three-dot icon in Cloud Storage section

# Exam Essentials

- **Databases:**
  - **Cloud SQL/Spanner:** Managed relational databases. Use SQL for querying.
  - **BigQuery:** Data warehouse, use SQL. Export data via console. Use `bq` command line.
  - **Cloud Datastore:** NoSQL, uses GQL.
  - **Cloud Bigtable:** NoSQL, wide-column. Use `cbt` command line.

- **Cloud Storage:**
  - **Convert Storage Classes:** Use gsutil rewrite.
  - **Move/Rename Objects:** Use `gsutil` or console.

- **Pub/Sub:**
  - **Message Queue:** Write to topics, receive via subscriptions (push/pull). Retention period for unread messages.

- **Cloud Dataproc:**
  - **Managed Spark/Hadoop:** For big data analytics and batch jobs. Use `gcloud dataproc` commands.

- **Command-Line Tools:**
  - **gcloud:** Most products
  - **gsutil:** Cloud Storage
  - **bq:** BigQuery
  - **cbt:** Cloud Bigtable
