# Chapter 11: Loading and Moving Data to Cloud Storage

## Cloud Storage

- **Create Bucket**: `gsutil mb gs://[BUCKET_NAME]/`
- **Upload File**: `gsutil cp [LOCAL_FILE] gs://[BUCKET_NAME]/`
- **Download File**: `gsutil cp gs://[BUCKET_NAME]/[FILE] [LOCAL_DIR]/`
- **Move Object**: `gsutil mv gs://[SOURCE_BUCKET]/[OBJECT] gs://[DESTINATION_BUCKET]/[OBJECT]`
- **Move Folder**: Use Console; cannot move folders via command line.

## Importing and Exporting Data

### Cloud SQL

- **Export SQL**: `gcloud sql export sql [INSTANCE_NAME] gs://[BUCKET]/[FILE] --database=[DB_NAME]`
- **Export CSV**: `gcloud sql export csv [INSTANCE_NAME] gs://[BUCKET]/[FILE] --database=[DB_NAME]`
- **Import SQL**: `gcloud sql import sql [INSTANCE_NAME] gs://[BUCKET]/[FILE] --database=[DB_NAME]`

### Cloud Datastore

- **Export**: `gcloud datastore export --namespaces="(default)" gs://[BUCKET]`
- **Import**: `gcloud datastore import gs://[BUCKET]/[PATH]/[FILE].overall_export_metadata`

### BigQuery

- **Export Table**: `bq extract --destination_format [FORMAT] --compression [TYPE] [DATASET].[TABLE] gs://[BUCKET]/[FILE]`
  - Formats: CSV, Avro, JSON
  - Compression: GZIP, deflate, snappy
- **Import Table**: 
  - **Console**: Create table from dataset, specify format and source.
  - **Command Line**: `bq load --autodetect --source_format=[FORMAT] [DATASET].[TABLE] [SOURCE_PATH]`

# Chapter 11: Importing and Exporting Data

## Cloud Spanner
- **Export Data:**
  - Console: Export form → Enter bucket, database, region.
  - Charges: Dataflow & egress charges.
- **Import Data:**
  - Console: Import form → Enter source bucket, destination database, region.
  - No `gcloud` command; use Dataflow (see [Cloud Dataflow docs](https://cloud.google.com/dataflow/docs/)).

## Cloud Bigtable
- **Export Data:**
  - Download JAR: `curl -f -O http://repo1.maven.org/maven2/com/google/cloud/bigtable/bigtablebeam-import/1.6.0/bigtable-beam-import-1.6.0-shaded.jar`
  - Command: 
    ```bash
    java -jar bigtable-beam-import-1.6.0-shaded.jar export \
    --runner=dataflow \
    --project=[PROJECT_ID] \
    --bigtableInstanceId=[INSTANCE_ID] \
    --bigtableTableId=[TABLE_ID] \
    --destinationPath=gs://[BUCKET_NAME]/[EXPORT_PATH] \
    --tempLocation=gs://[BUCKET_NAME]/[TEMP_PATH] \
    --maxNumWorkers=[10x_NUMBER_OF_NODES] \
    --zone=[DATAFLOW_JOB_ZONE]
    ```
- **Import Data:**
  - Command:
    ```bash
    java -jar bigtable-beam-import-1.6.0-shaded.jar import \
    --runner=dataflow \
    --project=[PROJECT_ID] \
    --bigtableInstanceId=[INSTANCE_ID] \
    --bigtableTableId=[TABLE_ID] \
    --sourcePattern='gs://[BUCKET_NAME]/[EXPORT_PATH]/part-*' \
    --tempLocation=gs://[BUCKET_NAME]/[TEMP_PATH] \
    --maxNumWorkers=[3x_NUMBER_OF_NODES] \
    --zone=[DATAFLOW_JOB_ZONE]
    ```

## Cloud Dataproc
- **Export Configuration:**
  - Command:
    ```bash
    gcloud beta dataproc clusters export [CLUSTER_NAME] --destination=[PATH_TO_EXPORT_FILE]
    ```
- **Import Configuration:**
  - Command:
    ```bash
    gcloud beta dataproc clusters import [SOURCE_FILE]
    ```

## Cloud Pub/Sub
- **Create Topic:**
  - Command:
    ```bash
    gcloud pubsub topics create [TOPIC_NAME]
    ```
- **Create Subscription:**
  - Command:
    ```bash
    gcloud pubsub subscriptions create --topic [TOPIC_NAME] [SUBSCRIPTION_NAME]
    ```
- **Publish Message:**
  - Command:
    ```bash
    gcloud pubsub topics publish [TOPIC_NAME] --message [MESSAGE]
    ```
- **Pull Message:**
  - Command:
    ```bash
    gcloud pubsub subscriptions pull --auto-ack [SUBSCRIPTION_NAME]
    ```

## Summary
- **Cloud Storage:**
  - Use `gsutil cp` to copy files; `gsutil acl ch -u` to change permissions.
- **Cloud SQL & BigQuery:**
  - Import/export via Console or `gcloud`.
- **Bigtable & Dataproc:**
  - Bigtable: Use Java JAR for import/export.
  - Dataproc: Export/import cluster configuration.
- **Pub/Sub:**
  - Decouples services, handles message queues.

